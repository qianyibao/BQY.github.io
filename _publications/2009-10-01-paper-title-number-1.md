---
title: "Improving Knowledge Distillation via Cross-Modal Insights from CLIP"
collection: publications
category: manuscripts
permalink: /publication/2025-01-01-clip-knowledge-distillation
excerpt: ''
date: 2025-01-01
venue: 'ICASSP (IEEE International Conference on Acoustics, Speech and Signal Processing, equivalent to SCI Q2)'
citation: 'BAO zhi qiang (2025). <i>ICASSP</i>.'
---

This study introduces cross-modal feature supervision into the knowledge distillation task by leveraging the cross-modal learning capability of the CLIP model, effectively improving the performance of lightweight models.
---
title: "Improving Knowledge Distillation via Cross-Modal Insights from CLIP"
collection: publications
category: manuscripts
permalink: /publication/2025-01-01-clip-knowledge-distillation
# excerpt: 'Enhancing knowledge distillation performance through cross-modal insights from CLIP, exploring the application value of cross-modal features in knowledge distillation tasks'
date: 2025-01-01
venue: 'ICASSP (IEEE International Conference on Acoustics, Speech and Signal Processing, equivalent to SCI Q2)'
citation: 'BAO zhi qinag (2025)'
---

This study introduces cross-modal feature supervision into the knowledge distillation task by leveraging the cross-modal learning capability of the CLIP model, effectively improving the performance of lightweight models.